\section{Data Processing and Feature Engineering}

A close look at our data reveals that we have ~10,000 rows and 83 columns. There were a lot of challenges associated with the data preprocessing like handling null values, categorical data and columns with duplicate data. Here, we show how we dealt with each of these challenges. 

\subsection{Data Cleaning}
	There were some invalid values, like "UNKNOWN", 'Unspecified" in various columns which were converted to nan before any other data preprocessing. 	

\subsection{Handling columns with null values}
	
	We deleted the following columns because more than 30\% of the values were null. Imputing these values in any form would just result in biasing the data. Also, it didn't seem like these columns had any relation with the other columns, so we couldn't apply any model to figure out these values based on the other values. 
	
	\begin{enumerate}
		\item DefaultBrowsersIdentifier 
		\item OrganizationIdentifier 
		\item PuaMode 
		\item SmartScreen (Semantically this seems useful, but any imputation here results in a bad accuracy) 
		\item Census\_ProcessorClass 
		\item Census\_InternalBatteryType
		\item Census\_IsFlightingInternal
		\item Census\_ThresholdOptIn
		\item Census\_IsWIMBootEnabled
	\end{enumerate}

	The following columns were deleted because they didn't convey a lot of information. The values here were either 0 or null. 
	
	\begin{enumerate}
		\item IsBeta
		\item AutoSampleOptIn
		\item SMode
		\item Census\_IsFlightsDisabled
		\item Census\_IsVirtualDevice
	\end{enumerate}

	On a side note, all the other columns which have been deleted are the ones which have been one hot encoded or have derived features. 
	
	The following columns were taken care of by imputing values. 
	
	\begin{enumerate}
		\item RtpStateBitfield 
		\item AVProductStatesIdentifier
		\item AVProductsInstalled
		\item AVProductsEnabled
		\item IsProtected
		\item Firewall
		\item UacLuaenable
		\item Census\_OEMNameIdentifier
		\item Census\_OEMModelIdentifier
		\item Census\_ProcessorCoreCount
		\item Census\_ProcessorManufacturerIdentifier
		\item Census\_ProcessorModelIdentifier
		\item Census\_PrimaryDiskTotalCapacity
		\item Census\_PrimaryDiskTypeName
		\item Census\_TotalPhysicalRAM
		\item Census\_InternalBatteryNumberOfCharges
		\item Census\_OSInstallLanguageIdentifier
		\item Census\_GenuineStateName
		\item Census\_FirmwareManufacturerIdentifier
		\item Census\_FirmwareVersionIdentifier
		\item Census\_IsAlwaysOnAlwaysConnectedCapable
		\item Census\_IsVirtualDevice
		\item Wdft\_IsGamer
		\item Wdft\_RegionIdentifier
	\end{enumerate}

	There were a few ways we could impute these values. Imputing with the mean did not make sense, because these values are discrete and specific. They are model numbers, identification numbers, etc. Mode seems to be a good choice for imputing the columns with specific numeric data.
	
	\begin{enumerate}
		\item Use mode everywhere
		\item For Categorical data, impute the null values with -1 and use mode for the numeric data
		\item Sample from the inverse probability distribution function. By doing so, the imputed values obey the underlying probability distribution which inherently biases the data the least. This method can be used irrespective of the kind of data - Categorical or numeric.
	\end{enumerate}

	We have use the third method to impute all the null values. This is done because the column's are roughly independent. PROVE WHYYYYYYYYYYYYYY \\ 
	
	The column Census\_SystemVolumeTotalCapacity was imputed based on Census\_PrimaryDiskTotalCapacity. Since there was a high correlation of the system disk being equal to the primary disk (there were no other disks), we made the same assumption for all the values which were missing from the column 'Census\_SystemVolumeTotalCapacity'. 
	
\subsection{Handling Categorical data}
	We have one hot encoded all columns with categorical data where the size of the domain wasn't very high. After a little bit of tweaking, we empirically arrived at the desired domain size, 22. Any more would result in too many columns, any less would lose out on a lot of data. After one hot encoding these columns, the data 
	
	The other categorical columns were then label encoded. Label encoding essentially gives every value a random number. There were some columns which while categorical, had a certain order to them. Label encoding these columns would lose the inherent 'order' they possess. Hence, we decided to customize the label encoding for them. 
	
	We now explain the customized label encodings, or feature engineering. 
	
	\subsubsection{Versions}
		All the version numbers were of the form $a.b.c.d$. This format was uniform throughout the five version columns - 'AvSigVersion', 'EngineVersion', 'AppVersion', 'Census\_OSVersion' and 'OsVer'. This format essentially means $major.minor[.build[.revision]]$.	We made a basic assumption that new update is better than it's predecessors i.e $2.0.1.0$ is better than $2.0.0.999$. To establish such an ordering, we converted the string into a number by multiplying the major, minor, build and revision numbers with the appropriate powers of $10$ and adding them up. 